{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MwRtqhfoEmJE"
   },
   "source": [
    "# **Hands-on Lab: Implementing the ETL (Extract, Transform, Load) Process**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_flGWYZjEygA"
   },
   "source": [
    "**Objective:**\n",
    "\n",
    "In this hands-on lab, students will learn how to implement the fundamental steps of the ETL process by extracting data from multiple sources, transforming the data, and loading it into a database. Students will use Python along with libraries such as Pandas for data transformation and PyMongo for loading the data into a MongoDB database.\n",
    "\n",
    "By the end of this lab, students will be able to:\n",
    "\n",
    "* Extract data from different sources (CSV and API).\n",
    "* Clean, transform, and validate the data.\n",
    "* Load the transformed data into MongoDB.\n",
    "* Automate the ETL process by building a reusable pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNqvAT4rFKjh"
   },
   "source": [
    "**Pre-requisites:**\n",
    "\n",
    "* Basic knowledge of Python.\n",
    "* MongoDB Atlas account (or a local MongoDB instance).\n",
    "* Install the required Python libraries:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Af48YJb-FwIV"
   },
   "source": [
    "**In this Lab:**\n",
    "\n",
    "You are tasked with creating an ETL pipeline for a fictitious retail company. You will extract product and sales data from different sources (a CSV file and a REST API), transform the data by cleaning and standardizing it, and load the transformed data into MongoDB for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdEXdoqcF4wf"
   },
   "source": [
    "**Step 1: Extract Data**\n",
    "\n",
    "**1.1. Extract Product Data from a CSV File**\n",
    "\n",
    "Create a CSV file named ***products.csv*** with the following data:\n",
    "\n",
    "product_id,product_name,category,price\n",
    "\n",
    "1001,Laptop,Electronics,1200\n",
    "\n",
    "1002,Smartphone,Electronics,800\n",
    "\n",
    "1003,Chair,Furniture,150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3C7Wvw-WGM9b"
   },
   "source": [
    "Use Python and Pandas to extract the product data from this CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1e-U7ChnGOSc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Product Data:\n",
      "   product_id product_name     category  price\n",
      "0        1001       Laptop  Electronics   1200\n",
      "1        1002   Smartphone  Electronics    800\n",
      "2        1003        Chair    Furniture    150\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extract data from the CSV file\n",
    "products_df = pd.read_csv('products.csv')\n",
    "print(\"Extracted Product Data:\")\n",
    "print(products_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftylwwhwGRT_"
   },
   "source": [
    "**1.2. Extract Sales Data from a REST API**\n",
    "\n",
    "For the sales data, we will simulate an API response using a dictionary. In a real-world scenario, you would use the requests library to fetch data from an API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mcwhIOGxGXok"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Sales Data:\n",
      "[{'sale_id': 'S001', 'product_id': '1001', 'quantity': 2, 'total': 2400}, {'sale_id': 'S002', 'product_id': '1002', 'quantity': 1, 'total': 800}, {'sale_id': 'S003', 'product_id': '1003', 'quantity': 4, 'total': 600}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Simulated API response (in a real scenario, use requests.get(URL).json())\n",
    "sales_data = [\n",
    "    {\"sale_id\": \"S001\", \"product_id\": \"1001\", \"quantity\": 2, \"total\": 2400},\n",
    "    {\"sale_id\": \"S002\", \"product_id\": \"1002\", \"quantity\": 1, \"total\": 800},\n",
    "    {\"sale_id\": \"S003\", \"product_id\": \"1003\", \"quantity\": 4, \"total\": 600}\n",
    "]\n",
    "\n",
    "print(\"Extracted Sales Data:\")\n",
    "print(sales_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QARgvzg4Gaem"
   },
   "source": [
    "**Step 2: Transform Data**\n",
    "\n",
    "**2.1. Clean and Standardize the Product Data**\n",
    "\n",
    "Use Pandas to clean and transform the product data. For this example, let's assume you need to ensure the price field is numeric and filter out products that are too expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "id": "bPQrKmOTGf8M",
    "outputId": "b085b850-37e2-4daf-c20a-25c0a7205057"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched Sales Data:\n",
      "  sale_id product_id  quantity  total product_name\n",
      "0    S001       1001         2   2400       Laptop\n",
      "1    S002       1002         1    800   Smartphone\n",
      "2    S003       1003         4    600        Chair\n"
     ]
    }
   ],
   "source": [
    "# Convert sales_data to a DataFrame\n",
    "sales_df = pd.DataFrame(sales_data)\n",
    "\n",
    "# Convert product_id to string in products_df\n",
    "products_df[\"product_id\"] = products_df[\"product_id\"].astype(str)\n",
    "\n",
    "#added by Georges to convert product_id to string\n",
    "sales_df[\"product_id\"]=sales_df[\"product_id\"].astype(str)\n",
    "\n",
    "# Join sales data with product data to add product_name\n",
    "sales_df = pd.merge(sales_df, products_df[['product_id', 'product_name']], on='product_id', how='left')\n",
    "print(\"Enriched Sales Data:\")\n",
    "print(sales_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWTlr7q-GiMB"
   },
   "source": [
    "**2.2. Enrich the Sales Data**\n",
    "\n",
    "For the sales data, we'll perform a simple enrichment by adding the product_name to each sale by joining the sales_data and products_df on the product_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-AE458wsGniG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched Sales Data:\n",
      "  sale_id product_id  quantity  total product_name\n",
      "0    S001       1001         2   2400       Laptop\n",
      "1    S002       1002         1    800   Smartphone\n",
      "2    S003       1003         4    600        Chair\n"
     ]
    }
   ],
   "source": [
    "# Convert sales_data to a DataFrame\n",
    "sales_df = pd.DataFrame(sales_data)\n",
    "\n",
    "# Join sales data with product data to add product_name\n",
    "sales_df = pd.merge(sales_df, products_df[['product_id', 'product_name']], on='product_id', how='left')\n",
    "print(\"Enriched Sales Data:\")\n",
    "print(sales_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_J9biezGzQE"
   },
   "source": [
    "**Step 3: Load Data into MongoDB**\n",
    "\n",
    "Now that the data is transformed and cleaned, load the product and sales data into MongoDB.\n",
    "\n",
    "**3.1. Connect to MongoDB**\n",
    "\n",
    "Ensure you have MongoDB running locally or use MongoDB Atlas. Connect to MongoDB using PyMongo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "J-WpayvQG3NP"
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "# Connect to MongoDB (replace <username> and <password> with your MongoDB Atlas credentials)\n",
    "#client = MongoClient(\"mongodb+srv://<username>:<password>@cluster0.mongodb.net/myFirstDatabase?retryWrites=true&w=majority\")\n",
    "#db = client['retail_db']\n",
    "\n",
    "connection_string=\"mongodb+srv://gassaf2:dbUserPassword@cluster0.xjx2q.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
    "# Connect to the MongoDB Atlas cluster\n",
    "client = MongoClient(connection_string)\n",
    "# Access a specific database called sales_db\n",
    "db = client['retails_db']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KdbvO_ynG5W3"
   },
   "source": [
    "**3.2. Load Product Data**\n",
    "\n",
    "Insert the transformed product data into the MongoDB products collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "yFPviZ0jG-hM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Product Data into MongoDB\n"
     ]
    }
   ],
   "source": [
    "# Convert DataFrame to dictionary and insert into MongoDB\n",
    "product_records = products_df.to_dict(orient='records')\n",
    "db.products.insert_many(product_records)\n",
    "print(\"Loaded Product Data into MongoDB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RcPF_GfEHBcc"
   },
   "source": [
    "**3.3. Load Sales Data**\n",
    "\n",
    "Insert the enriched sales data into the MongoDB sales collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "xGqJWM2sHHSh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Sales Data into MongoDB\n"
     ]
    }
   ],
   "source": [
    "# Convert DataFrame to dictionary and insert into MongoDB\n",
    "sales_records = sales_df.to_dict(orient='records')\n",
    "db.sales.insert_many(sales_records)\n",
    "print(\"Loaded Sales Data into MongoDB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0w-Iu3KuHKGg"
   },
   "source": [
    "**Step 4: Automate the ETL Process**\n",
    "\n",
    "To make the ETL process reusable, wrap the steps into functions and run the ETL pipeline from start to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "p-h6IeqiHRIZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETL Process Completed!\n"
     ]
    }
   ],
   "source": [
    "def extract_products():\n",
    "    return pd.read_csv('products.csv')\n",
    "\n",
    "def extract_sales():\n",
    "    return pd.DataFrame(sales_data)\n",
    "\n",
    "def transform_products(products_df):\n",
    "    products_df['price'] = pd.to_numeric(products_df['price'], errors='coerce')\n",
    "    return products_df[products_df['price'] < 1000]\n",
    "\n",
    "def transform_sales(sales_df, products_df):\n",
    "    # Convert product_id to string in products_df\n",
    "    products_df[\"product_id\"] = products_df[\"product_id\"].astype(str)\n",
    "\n",
    "    #added by Georges to convert product_id to string\n",
    "    sales_df[\"product_id\"]=sales_df[\"product_id\"].astype(str)\n",
    "    return pd.merge(sales_df, products_df[['product_id', 'product_name']], on='product_id', how='left')\n",
    "\n",
    "def load_data(products_df, sales_df):\n",
    "    db.products.insert_many(products_df.to_dict(orient='records'))\n",
    "    db.sales.insert_many(sales_df.to_dict(orient='records'))\n",
    "\n",
    "# Run the ETL pipeline\n",
    "products_df = extract_products()\n",
    "sales_df = extract_sales()\n",
    "transformed_products_df = transform_products(products_df)\n",
    "transformed_sales_df = transform_sales(sales_df, products_df)\n",
    "load_data(transformed_products_df, transformed_sales_df)\n",
    "print(\"ETL Process Completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJ2xNTfeEjAk"
   },
   "source": [
    "**Conclusion:**\n",
    "This hands-on lab provides a comprehensive introduction to the ETL process, from extracting raw data from multiple sources, transforming it for quality and consistency, and finally loading it into MongoDB."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
